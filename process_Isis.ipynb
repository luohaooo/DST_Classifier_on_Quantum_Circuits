{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c9a957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_table('iris.data', header=None)\n",
    "raw_data = np.array(df)\n",
    "pro_data = [[],[],[]]\n",
    "for item in raw_data:\n",
    "    pro_item = str(item[0]).split(',')\n",
    "    pro_item = [eval(pro_item[0]),eval(pro_item[1]),eval(pro_item[2]),eval(pro_item[3]),pro_item[4]]\n",
    "    if pro_item[4] == 'Iris-setosa':    \n",
    "        pro_item[4] = 0\n",
    "        pro_data[0].append(pro_item)\n",
    "    if pro_item[4] == 'Iris-versicolor': \n",
    "        pro_item[4] = 1\n",
    "        pro_data[1].append(pro_item)\n",
    "    if pro_item[4] == 'Iris-virginica':  \n",
    "        pro_item[4] = 2\n",
    "        pro_data[2].append(pro_item)\n",
    "pro_data = np.array(pro_data)\n",
    "\n",
    "train_ratio = 0.9\n",
    "def train_test(all_data, train_ratio):\n",
    "    train_len = round(len(all_data)*train_ratio)\n",
    "    np.random.shuffle(all_data)\n",
    "    train_data = all_data[0:train_len]\n",
    "    test_data = all_data[train_len:len(all_data)]\n",
    "    return[train_data,test_data]\n",
    "# print(train_test(pro_data[0], train_ratio))\n",
    "m = 4 # attribute\n",
    "n = 3 # types\n",
    "# save training and testing data\n",
    "training_set = [[[] for col in range(m)] for row in range(n)]\n",
    "testing_set = [[] for row in range(n)]\n",
    "# for each type\n",
    "for ii in range(n):\n",
    "    x = train_test(pro_data[ii],train_ratio)\n",
    "    testing_set[ii] = x[1][:,0:4]\n",
    "#     for each attribute\n",
    "    for jj in range(m):\n",
    "        training_set[ii][jj] = x[0][:,jj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8da2797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM iteration to estimate GMM coefficients with training data set\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# GMM coefficients\n",
    "def learn_GMM_coefficients(training_set, components_num):\n",
    "    n = len(training_set)\n",
    "    m = len(training_set[0])\n",
    "    weights = [[[[] for item in range(components_num)] for col in range(m)] for row in range(n)]\n",
    "    means = [[[[] for item in range(components_num)] for col in range(m)] for row in range(n)]\n",
    "    covariances = [[[[] for item in range(components_num)] for col in range(m)] for row in range(n)]\n",
    "    for ii in range(n):\n",
    "        for jj in range(m):\n",
    "            gmm_coefficients = GaussianMixture(n_components=3).fit(training_set[ii][jj].reshape(-1, 1))\n",
    "            for item in range(components_num):\n",
    "                weights[ii][jj][item] = gmm_coefficients.weights_[item]\n",
    "                means[ii][jj][item] = gmm_coefficients.means_[item][0]\n",
    "                covariances[ii][jj][item] = gmm_coefficients.covariances_[item][0]\n",
    "    return weights, means, covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "303aa711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot GMM curves\n",
    "np.mean([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f3a9bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BPAs\n",
    "def calc_Gaussian(mean,cov,x):\n",
    "    return (1/np.sqrt(2*np.pi*cov))*np.exp(-np.square(x-mean)/(2*cov))\n",
    "\n",
    "def calc_mix_Gaussian(weights,means,covs,x):\n",
    "    components_num = len(weights)\n",
    "    prob = 0\n",
    "    for item in range(components_num):\n",
    "        prob += weights[item]*calc_Gaussian(means[item],covs[item],x)\n",
    "    return prob[0]\n",
    "\n",
    "def generate_BPAs(x, weights, means, covariances):\n",
    "    # x: input testing data (4 atrributes)\n",
    "    attributes_num = len(x)\n",
    "    types_num = len(means)\n",
    "    # BPAs for each attribute\n",
    "    BPAs = [[] for attribute in range(attributes_num)]\n",
    "    alphas = [[] for attribute in range(attributes_num)]\n",
    "    for attribute in range(attributes_num):\n",
    "        attribute_value = x[attribute]\n",
    "        # calculate GMM distribution value\n",
    "        f = [[] for type in range(types_num)]\n",
    "        for type in range(types_num):\n",
    "            f[type] = calc_mix_Gaussian(weights[type][attribute],means[type][attribute],covariances[type][attribute],attribute_value)\n",
    "        f = np.array(f)\n",
    "        # calculate pi_0 and pi_1\n",
    "        pi1 = f/max(f)\n",
    "        pi0 = np.ones(types_num)-pi1\n",
    "        # calculate the rotation angles\n",
    "        alpha = np.arctan((pi1+1e-100)/(pi0+1e-100))\n",
    "        alphas[attribute] = alpha\n",
    "        # calculate BPA and record\n",
    "        dimension = 2**types_num\n",
    "        BPA = [[] for ii in range(dimension)]\n",
    "        for ii in range(dimension):\n",
    "            # binary represenation\n",
    "            bits = \"{:0>10b}\".format(ii)\n",
    "            # mass\n",
    "            m = 1\n",
    "            for jj in range(types_num):\n",
    "                bit = bits[-(jj+1)]\n",
    "                if bit == '1':\n",
    "                    m *= pi1[jj]\n",
    "                if bit == '0':\n",
    "                    m *= pi0[jj]\n",
    "            BPA[ii] = m\n",
    "        BPAs[attribute] = BPA\n",
    "    return BPAs, alphas\n",
    "\n",
    "# BPA, alphas = generate_BPAs(testing_set[2][0], weights, means, covariances)\n",
    "# BPA\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "dbba2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining BPAs on quantum circuits and conduct measurement\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit import Aer, transpile\n",
    "\n",
    "\n",
    "def combine_qubits(circ, input1, input2, output):\n",
    "    # x and y (same dimention) are input qubits to be combined by CCR\n",
    "    # circ is the quantum circuits\n",
    "    # z is the output qubits for the combined result\n",
    "    qubits_num = len(input1)\n",
    "    for qubit in range(qubits_num):\n",
    "        circ.ccx(input1[qubit],input2[qubit],output[qubit])\n",
    "\n",
    "\n",
    "def combineBPAs_quantum(alphas, shots):\n",
    "    attributes_num = len(alphas)\n",
    "    types_num = len(alphas[0])\n",
    "\n",
    "    # establish quantum circuits\n",
    "    qubits_num = (2*attributes_num-1)*types_num\n",
    "    circ = QuantumCircuit(qubits_num)\n",
    "    # build quantum BPAs\n",
    "    for attribute in range(attributes_num):\n",
    "        for type in range(types_num):\n",
    "            circ.ry(2*alphas[attribute][type],attribute*types_num+type)\n",
    "    # combine\n",
    "    combine_qubits(circ, [0,1,2],[3,4,5],[12,13,14])\n",
    "    combine_qubits(circ, [6,7,8],[12,13,14],[15,16,17])\n",
    "    combine_qubits(circ, [9,10,11],[15,16,17],[18,19,20])\n",
    "\n",
    "    # measurement\n",
    "    meas = QuantumCircuit(21, 3)\n",
    "    meas.barrier(range(21))\n",
    "    # map the quantum measurement to the classical bits\n",
    "    meas.measure(range(18,21), range(3))\n",
    "    circ.add_register(meas.cregs[0])\n",
    "    qc = circ.compose(meas)\n",
    "    backend_sim = Aer.get_backend('qasm_simulator')\n",
    "    job_sim = backend_sim.run(transpile(qc, backend_sim), shots=shots)\n",
    "    result_sim = job_sim.result()\n",
    "    counts = result_sim.get_counts(qc)\n",
    "    # print(counts)\n",
    "\n",
    "    # process the measurement result and output\n",
    "    dimension = 2**types_num\n",
    "    combined_BPA = [0 for item in range(dimension)]\n",
    "    for key in counts.keys():\n",
    "        value = counts[key]/shots\n",
    "        index = int(key,2)\n",
    "        combined_BPA[index] = value\n",
    "\n",
    "    return combined_BPA\n",
    "\n",
    "# x = combineBPAs_quantum(alphas, 1024)\n",
    "# print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bc09dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision-making by the combined BPA\n",
    "import math\n",
    "def decision_making(BPA):\n",
    "    dimension = len(BPA)\n",
    "    types_num = round(math.log2(dimension))\n",
    "    prob = [0 for ii in range(types_num)]\n",
    "    for index in range(dimension):\n",
    "        if BPA[index] != 0:\n",
    "            mass = BPA[index]\n",
    "            bin_index = \"{:0>10b}\".format(index)\n",
    "            set_num = 0\n",
    "            for ii in range(types_num):\n",
    "                if bin_index[-(ii+1)] == '1':\n",
    "                    set_num += 1\n",
    "            if set_num != 0:\n",
    "                mass_divided = mass/set_num\n",
    "                for ii in range(types_num):\n",
    "                    if bin_index[-(ii+1)] == '1':\n",
    "                        prob[ii] += mass_divided\n",
    "    return prob.index(max(prob))\n",
    "\n",
    "# decision_making(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e710012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\29265\\.conda\\envs\\quenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiments\n",
    "def traning_testing(training_set, testing_set, components_num, shots):\n",
    "    testing_num = 0\n",
    "    error_num = 0\n",
    "    types_num = len(testing_set)\n",
    "\n",
    "    weights, means, covariances = learn_GMM_coefficients(training_set, components_num)\n",
    "     \n",
    "    for ii in range(types_num):\n",
    "        testing_num += len(testing_set[ii])\n",
    "        for testing_data in testing_set[ii]:\n",
    "            BPA, alphas = generate_BPAs(testing_data, weights, means, covariances)\n",
    "            combined_BPA = combineBPAs_quantum(alphas, shots)\n",
    "            classification_result = decision_making(combined_BPA)\n",
    "            if  classification_result != ii:\n",
    "                error_num += 1\n",
    "                print(classification_result, ii)\n",
    "    accuracy = 1 - error_num/testing_num\n",
    "    return accuracy\n",
    "\n",
    "traning_testing(training_set, testing_set, 3, 1024) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
